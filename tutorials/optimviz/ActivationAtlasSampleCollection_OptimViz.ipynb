{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActivationAtlasSampleCollection_OptimViz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2PKna21WLK"
      },
      "source": [
        "# Collecting Samples for Activation Atlases with captum.optim\n",
        "\n",
        "This notebook demonstrates how to collect the activation and corresponding attribution samples required for [Activation Atlases](https://distill.pub/2019/activation-atlas/) for the InceptionV1 model imported from Caffe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6T6jxWb4cil"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from captum.optim.models import googlenet\n",
        "\n",
        "import captum.optim as opt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtE-t6ZG0-sJ"
      },
      "source": [
        "### Dataset Download & Setup \n",
        "\n",
        "To begin, we'll need to download and setup the image dataset that our model was trained on. You can download ImageNet's ILSVRC2012 dataset from the [ImageNet website](http://www.image-net.org/challenges/LSVRC/2012/) or via BitTorrent from [Academic Torrents](https://academictorrents.com/details/a306397ccf9c2ead27155983c254227c0fd938e2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDt-6WMp0qh3"
      },
      "source": [
        "collect_attributions = True  # Set to False for no attributions\n",
        "\n",
        "# Setup basic transforms\n",
        "# The model has the normalization step in its internal transform_input\n",
        "# function, so we don't need to normalize our inputs here.\n",
        "transform_list = [\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "]\n",
        "transform_list = torchvision.transforms.Compose(transform_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i85yBIhL7owj"
      },
      "source": [
        "To make it easier to load the ImageNet dataset, we can use [Torchvision](https://pytorch.org/vision/stable/datasets.html#imagenet)'s `torchvision.datasets.ImageNet` instead of the default `ImageFolder`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oRqxlMq7gJ4"
      },
      "source": [
        "# Load the dataset\n",
        "image_dataset = torchvision.datasets.ImageNet(\n",
        "    root=\"path/to/dataset\", split=\"train\", transform=transform_list\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573290Fr8KN7"
      },
      "source": [
        "Now we wrap our dataset in a `torch.utils.data.DataLoader` instance, and set the desired batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCfwsvR7iGC"
      },
      "source": [
        "# Set desired batch size & load dataset with torch.utils.DataLoader\n",
        "image_loader = torch.utils.data.DataLoader(\n",
        "    image_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qfpBAPu18jv"
      },
      "source": [
        "We load our model, then set the desired model target layers and corresponding file names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMViqsJ82Mcp"
      },
      "source": [
        "# Model to collect samples from, what layers of the model to collect samples from,\n",
        "# and the desired names to use for the target layers.\n",
        "sample_model = (\n",
        "    googlenet(\n",
        "        pretrained=True, replace_relus_with_redirectedrelu=False, bgr_transform=True\n",
        "    )\n",
        "    .eval()\n",
        "    .to(device)\n",
        ")\n",
        "sample_targets = [sample_model.mixed4c_relu]\n",
        "sample_target_names = [\"mixed4c_relu_samples\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl719nyZEGSt"
      },
      "source": [
        "By default the activation samples will not have the right class attributions, so we remedy this by loading a second instance of our model. We then replace all `nn.MaxPool2d` layers in the second model instance with Captum's `MaxPool2dRelaxed` layer. The relaxed max pooling layer lets us estimate the sample class attributions by determining the rate at which increasing the neuron affects the output classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-VJyHRm1tqC"
      },
      "source": [
        "# Optionally collect attributions from a copy of the first model that's\n",
        "# been setup with relaxed pooling layers.\n",
        "if collect_attributions:\n",
        "    sample_model_attr = (\n",
        "        googlenet(\n",
        "            pretrained=True, replace_relus_with_redirectedrelu=False, bgr_transform=True\n",
        "        )\n",
        "        .eval()\n",
        "        .to(device)\n",
        "    )\n",
        "    opt.models.replace_layers(\n",
        "        sample_model_attr,\n",
        "        torch.nn.MaxPool2d,\n",
        "        opt.models.MaxPool2dRelaxed,\n",
        "        transfer_vars=True,\n",
        "    )\n",
        "    sample_attr_targets = [sample_model_attr.mixed4c_relu]\n",
        "    sample_logit_target = sample_model_attr.fc\n",
        "else:\n",
        "    sample_model_attr = None\n",
        "    sample_attr_targets = None\n",
        "    sample_logit_target = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zDGSR5-qDW"
      },
      "source": [
        "With our dataset loaded and models ready to go, we can now start collecting our samples. To make sample collection easier, we can use Captum's `capture_activation_samples` function to randomly sample an x and y position for every image for all specified target layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uODdkyjY1lap"
      },
      "source": [
        "# Directory to save sample files to\n",
        "sample_dir = \"inceptionv1_samples\"\n",
        "try:\n",
        "    os.mkdir(sample_dir)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Collect samples & optionally attributions as well\n",
        "opt.dataset.capture_activation_samples(\n",
        "    loader=image_loader,\n",
        "    model=sample_model,\n",
        "    targets=sample_targets,\n",
        "    target_names=sample_target_names,\n",
        "    attr_model=sample_model_attr,\n",
        "    attr_targets=sample_attr_targets,\n",
        "    input_device=device,\n",
        "    sample_dir=sample_dir,\n",
        "    show_progress=True,\n",
        "    collect_attributions=collect_attributions,\n",
        "    logit_target=sample_logit_target,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMrBUaPi97fF"
      },
      "source": [
        "Now that we've collected our samples, we need to combine them into a single tensor. Below we use the `consolidate_samples` function to load each list of tensor samples, and then concatinate them into a single tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKUPszVR1Ew-"
      },
      "source": [
        "# Combine our newly collected samples into single tensors.\n",
        "# We load the sample tensors from sample_dir and then\n",
        "# concatenate them.\n",
        "\n",
        "for name in sample_target_names:\n",
        "    activation_samples = opt.dataset.consolidate_samples(\n",
        "        sample_dir=sample_dir,\n",
        "        sample_basename=name + \"_activations\",\n",
        "        dim=1,\n",
        "        show_progress=True,\n",
        "    )\n",
        "    if collect_attributions:\n",
        "        sample_attributions = opt.dataset.consolidate_samples(\n",
        "            sample_dir=sample_dir,\n",
        "            sample_basename=name + \"_attributions\",\n",
        "            dim=0,\n",
        "            show_progress=True,\n",
        "        )\n",
        "\n",
        "    # Save the results\n",
        "    torch.save(activation_samples, name + \"activation_samples.pt\")\n",
        "    if collect_attributions:\n",
        "        torch.save(sample_attributions, name + \"attribution_samples.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}